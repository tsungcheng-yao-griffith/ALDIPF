# -*- coding: utf-8 -*-
"""RoBERTa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lahH0HV3u-CHJfzFivSRpDEolZPz_SdS
"""

from google.colab import drive

from google.colab import drive

drive.mount('/content/drive')

!pip install accelerate>=0.21.0
!pip install transformers[torch]
!pip install datasets transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset, Dataset
import torch
import pandas as pd
import io

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset, Dataset
import torch
import pandas as pd
import io

# Load the training and testing datasets using pandas
df1 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DistilBERT/training_15_unfolded.csv")
df2 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DistilBERT/testing_15_unfolded.csv")
df1 = Dataset.from_pandas(df1)
df2 = Dataset.from_pandas(df2)

# Convert the pandas DataFrames to dictionaries
train_data = {"train": df1}
test_data = {"test": df2}

tokenizer = AutoTokenizer.from_pretrained("roberta-base")

special_tokens_dict = {
    'additional_special_tokens': [
        '[Flag_Very_Low_Rationality]', '[Flag_Low_Rationality]', '[Flag_Average_Rationality]',
        '[Flag_Average_Lower_Rationality]', '[Flag_Average_Higher_Rationality]', '[Flag_High_Rationality]',
        '[Flag_Very_High_Rationality]', '[Flag_Very_Low_Irrationality]', '[Flag_Low_Irrationality]',
        '[Flag_Average_Irrationality]', '[Flag_Average_Lower_Irrationality]', '[Flag_Average_Higher_Irrationality]',
        '[Flag_High_Irrationality]', '[Flag_Very_High_Irrationality]', '[Flag_Very_Low_SD]', '[Flag_Low_SD]',
        '[Flag_Average_SD]', '[Flag_Average_Lower_SD]', '[Flag_Average_Higher_SD]', '[Flag_High_SD]',
        '[Flag_Very_High_SD]', '[Flag_Ex_High_SD]'
    ]
}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2)
model.resize_token_embeddings(len(tokenizer))

def tokenize_batch(batch):
    return tokenizer(batch["text"], padding=True, truncation=True, return_tensors="pt")

tokenized_dataset = df1.map(tokenize_batch, batched=True)
tokenized_dataset2 = df2.map(tokenize_batch, batched=True)

def inspect_tokenization(text):
    tokens = tokenizer.tokenize(text)
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    print(f"Text: {text}")
    print(f"Tokens: {tokens}")
    print(f"Token IDs: {token_ids}")

tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
tokenized_dataset2.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
model.to("cuda")

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    weight_decay=0.01,
    num_train_epochs=10,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    gradient_accumulation_steps=2,
    lr_scheduler_type="linear",
    warmup_steps=500,
    fp16=True
)

from transformers import DataCollatorWithPadding, EarlyStoppingCallback

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset2,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
)

trainer.train()

model.save_pretrained("/content/drive/MyDrive/conventional_roberta_model_unfolded")
tokenizer.save_pretrained("/content/drive/MyDrive/conventional_roberta_model_token_unfolded")